{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azizamirsaidova/abstractive_text_summarization/blob/main/Text_summarization_pre_processing_and_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcWg6hWuvpn_",
        "outputId": "8ed2084a-1898-46d3-84e7-0c41f132ed0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'cnn_dailymail' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/datasets/ccdv/cnn_dailymail\n",
        "!pip install tensorflow==1.13.1\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5O6OyQHzvHI",
        "outputId": "dec8e809-c120-4612-ca5d-c8e3ae154434"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('train.csv',error_bad_lines=False, engine=\"python\")\n",
        "df = df.drop(['id'],1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWqjpCX51hx3",
        "outputId": "35085dec-4e2c-43d6-b797-e55b4053184b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review # 1\n",
            "Original Text : \n",
            "By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\n",
            "\n",
            "\n",
            "Summary Text : \n",
            "Bishop John Folda, of North Dakota, is taking time off after being diagnosed .\n",
            "He contracted the infection through contaminated food in Italy .\n",
            "Church members in Fargo, Grand Forks and Jamestown could have been exposed .\n",
            "===========================================================================================================\n",
            "\n",
            "\n",
            "Review # 2\n",
            "Original Text : \n",
            "(CNN) -- Ralph Mata was an internal affairs lieutenant for the Miami-Dade Police Department, working in the division that investigates allegations of wrongdoing by cops. Outside the office, authorities allege that the 45-year-old longtime officer worked with a drug trafficking organization to help plan a murder plot and get guns. A criminal complaint unsealed in U.S. District Court in New Jersey Tuesday accuses Mata, also known as \"The Milk Man,\" of using his role as a police officer to help the drug trafficking organization in exchange for money and gifts, including a Rolex watch. In one instance, the complaint alleges, Mata arranged to pay two assassins to kill rival drug dealers. The killers would pose as cops, pulling over their targets before shooting them, according to the complaint. \"Ultimately, the (organization) decided not to move forward with the murder plot, but Mata still received a payment for setting up the meetings,\" federal prosecutors said in a statement. The complaint also alleges that Mata used his police badge to purchase weapons for drug traffickers. Mata, according to the complaint, then used contacts at the airport to transport the weapons in his carry-on luggage on trips from Miami to the Dominican Republic. Court documents released by investigators do not specify the name of the drug trafficking organization with which Mata allegedly conspired but says the organization has been importing narcotics from places such as Ecuador and the Dominican Republic by hiding them \"inside shipping containers containing pallets of produce, including bananas.\" The organization \"has been distributing narcotics in New Jersey and elsewhere,\" the complaint says. Authorities arrested Mata on Tuesday in Miami Gardens, Florida. It was not immediately clear whether Mata has an attorney, and police officials could not be immediately reached for comment. Mata has worked for the Miami-Dade Police Department since 1992, including directing investigations in Miami Gardens and working as a lieutenant in the K-9 unit at Miami International Airport, according to the complaint. Since March 2010, he had been working in the internal affairs division. Mata faces charges of aiding and abetting a conspiracy to distribute cocaine, conspiring to distribute cocaine and engaging in monetary transactions in property derived from specified unlawful activity. He is scheduled to appear in federal court in Florida on Wednesday. If convicted, Mata could face life in prison. CNN's Suzanne Presto contributed to this report.\n",
            "\n",
            "\n",
            "Summary Text : \n",
            "Criminal complaint: Cop used his role to help cocaine traffickers .\n",
            "Ralph Mata, an internal affairs lieutenant, allegedly helped group get guns .\n",
            "He also arranged to pay two assassins in a murder plot, a complaint alleges .\n",
            "===========================================================================================================\n",
            "\n",
            "\n",
            "Review # 3\n",
            "Original Text : \n",
            "A drunk driver who killed a young woman in a head-on crash while checking his mobile phone has been jailed for six years. Craig Eccleston-Todd, 27, was driving home from a night at a pub when he received a text message. As he was reading or replying to it, he veered across the road while driving round a bend and smashed into Rachel Titley’s car coming the other way. Craig Eccleston-Todd, 27 (left) was using his mobile phone when he crashed head-on into the car being driven by Rachel Titley, 28 (right). She died later from her injuries . The head-on crash took place in October 2013. Mr Eccleston-Todd's car was barely recognisable (pictured) Police said Eccleston-Todd had drunk at least three or four pints of beer before getting behind the wheel. He was found guilty of causing death by dangerous driving at Portsmouth Crown Court yesterday. Miss Titley, a 28-year-old solicitor’s clerk from Cowes, Isle of Wight, had also spent the evening with friends at a pub but had not drunk any alcohol, police said. She was driving responsibly and there was ‘nothing she could have done to avoid the collision’, they added. Lindsay Pennell, prosecuting, said: ‘Craig Eccleston-Todd’s driving resulted in the tragic death of a young woman, Rachel Titley, a death that could have been avoided. ‘Mr Eccleston-Todd took the decision to pick up his mobile phone whilst driving and, either reading or replying to this text message, was so distracted that he failed to negotiate a left-hand bend, crossing the central white line into the path of Miss Titley’s oncoming car. Miss Titley was pulled the wreckage of her Daihatsu Cuore but died later from her injuries in hospital . ‘Miss Titley [had] a bright future ahead of her. She was also returning home having spent an enjoyable evening with friends and was driving responsibly. ‘She had arranged to contact her friends when she got home to confirm that she had arrived safely. Her friends sadly never heard from her after they parted company. ‘Miss Titley’s death in these circumstances reiterates the danger of using a hand-held mobile phone whilst driving.’ Police were unable to take breath or blood tests from Eccleston-Todd immediately, but in tests several hours after the accident he was only marginally under the drink-drive limit. The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His phone records showed he was also texting around the time of the crash. PC Mark Furse, from Hampshire constabulary’s serious collision investigation unit, said: 'Our thoughts are with Rachel's family at this time. She had been out with friends at a pub in Shalfleet that evening, but had not had any alcohol. 'Our investigation showed that there was nothing she could have done to avoid the collision and sadly it cost her her life. 'Mr Eccleston-Todd had left work in Yarmouth and met with friends at a pub where he drank at least three to four pints of lager. He hadn't long left the pub to return home when the collision occurred at around 9.30pm. 'We weren't able to take breath or blood tests from him immediately and although blood taken several hours after the collision showed he was marginally under the limit, we maintain he would have been over the limit at the time of the collision and in summing up today, the judge agreed. 'The analysis of his phone records showed that he was texting on his phone around the time of the collision so it's highly likely this would also have contributed to his dangerous driving and loss of control.' Eccleston-Todd was found guilty of causing death by dangerous driving following a trial at Portsmouth Crown Court (pictured) He added: 'Mr Eccleston-Todd will now spend six years behind bars, but Rachel's family have lost her forever. 'I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once they're on the road. 'The dangers of drink driving and driving whilst using a mobile phone are obvious. Those who continue to do so risk spending a substantial time in prison. This case highlights just how tragic the consequences of committing these offences can be.' ‘Mr Eccleston-Todd will now spend six years behind bars, but Rachel’s family have lost her for ever. I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once they’re on the road. This case highlights just how tragic the consequences of committing these offences can be.’ Eccleston-Todd, of Newport, Isle of Wight, was also disqualified from driving for eight years after which he will have to complete an extended re-test.\n",
            "\n",
            "\n",
            "Summary Text : \n",
            "Craig Eccleston-Todd, 27, had drunk at least three pints before driving car .\n",
            "Was using phone when he veered across road in Yarmouth, Isle of Wight .\n",
            "Crashed head-on into 28-year-old Rachel Titley's car, who died in hospital .\n",
            "Police say he would have been over legal drink-drive limit at time of crash .\n",
            "He was found guilty at Portsmouth Crown Court of causing death by dangerous driving .\n",
            "===========================================================================================================\n",
            "\n",
            "\n",
            "Review # 4\n",
            "Original Text : \n",
            "(CNN) -- With a breezy sweep of his pen President Vladimir Putin wrote a new chapter into Crimea's turbulent history, committing the region to a future returned to Russian domain. Sixty years prior, Ukraine's breakaway peninsula was signed away just as swiftly by Soviet leader Nikita Khrushchev. But dealing with such a blatant land grab on its eastern flank won't be anywhere near as quick and easy for Europe's 28-member union. Because, unlike Crimea's rushed referendum, everyone has a say. After initially slapping visa restrictions and asset freezes on a limited number of little known politicians and military men, Europe is facing urgent calls to widen the scope of its measures to target the Russian business community in particular. The logic of this is that those who run Russia and own it are essentially two sides of the coin. Alexei Navalny, one-time Moscow mayoral contender now under house arrest for opposing the current regime, called for Europe's leaders to ban everyone -- from Vladimir Putin's personal banker to Chelsea Football Club owner Roman Abramovich from keeping their money and loved ones abroad. Asset freezes and visa restrictions are especially palatable options for the EU because they can be rolled out on a discretionary basis, without requiring cumbersome legal procedures and recourse. In fact Russia cancels visas for people it doesn't like all the time. Just look at Hermitage Capital founder Bill Browder who lost both his right of entry and Moscow-based money in 2005 and dare not go back. Russia also banned the adoption of its orphans by Americans in retaliation for the US's implementation of an anti-corruption law named after Sergei Magnitsky, Browder's lawyer who died after a year in a Moscow detention center, apparently beaten to death. Yet in playing the 'money talks' card, Europe must be ready for the consequences of such action. Because money also walks. As such EU leaders must be ready to accept sanctions are a two-way street and will hurt both sides. Targeting Russia's peripatetic business community would be one way of sapping their tenuous support for President Putin. And such a strategy might also turn out to have a silver lining: awarding EU countries a chance to finally deal with some of the more unpleasant sides of their patronage, including money laundering and corruption, which have inflated prize assets like London property and Picasso paintings for years. Where Europe should hold fire though is trade. Two decades of post-Soviet rapprochement and almost $500 billion worth of commerce is a lot to put at stake. It's true that any trade war would hurt Russia far harder than it would the EU - not least because 15% of the former's GDP comes from exports to the bloc. But Europe - with its hefty reliance on Russian gas - would have a hard time keeping its factories going and citizens warm without power from the east. And while Putin flexes his political muscle, open trade channels keep the dialogue going giving all sides a chance to change the subject and talk less tensely. No one can afford to cut off that lifeline, especially now with Europe's economy on the rebound and Russia's one on the wane.\n",
            "\n",
            "\n",
            "Summary Text : \n",
            "Nina dos Santos says Europe must be ready to accept sanctions will hurt both sides .\n",
            "Targeting Russia's business community would be one way of sapping their support for President Putin, she says .\n",
            "But she says Europe would have a hard time keeping its factories going without power from the east .\n",
            "===========================================================================================================\n",
            "\n",
            "\n",
            "Review # 5\n",
            "Original Text : \n",
            "Fleetwood are the only team still to have a 100% record in Sky Bet League One as a 2-0 win over Scunthorpe sent Graham Alexander’s men top of the table. The Cod Army are playing in the third tier for the first time in their history after six promotions in nine years and their remarkable ascent shows no sign of slowing with Jamie Proctor and Gareth Evans scoring the goals at Glanford Park. Fleetwood were one of five teams to have won two out of two but the other four clubs - Peterborough, Bristol City, Chesterfield and Crawley - all hit their first stumbling blocks. Posh were defeated 2-1 by Sheffield United, who had lost both of their opening contests. Jose Baxter’s opener gave the Blades a first-half lead, and although it was later cancelled out by Shaun Brisley’s goal, Ben Davies snatched a winner six minutes from time. In the lead: Jose Baxter (right) celebrates opening the scoring for Sheffield United . Up for the battle: Sheffield United's Michael Doyle (left) challenges Peterborough's Kyle Vassell in a keenly-contested clash . Bristol City, who beat Nigel Clough’s men on the opening day, were held to a goalless draw by last season's play-off finalists Leyton Orient while Chesterfield, the League Two champions, were beaten 1-0 by MK Dons, who play Manchester United in the Capital One Cup in seven days’ time. Arsenal loanee Benik Afobe scored the only goal of the game just after the break. Meanwhile, Crawley lost their unbeaten status, while Bradford maintained theirs, thanks to a 3-1 win for the Bantams. James Hanson became the first player to score against Crawley this season after 49 minutes before Joe Walsh equalised five minutes later. Heads up: Bristol City's Korey Smith (left) and Leyton Orient's Lloyd James go up for a header . But strikes from Billy Knott and Mason Bennett sealed an impressive away win Phil Parkinson's men. Bradford are now second behind Fleetwood after Doncaster’s stoppage-time equaliser meant Preston, for whom Joe Garner signed a new contract earlier on Tuesday, were held to a 1-1 draw which slipped them down the table. Chris Humphrey looked to have secured the points for the Lilywhites but Nathan Tyson struck a last-gasp leveller. Stand-in striker Matt Done scored a hat-trick for Rochdale in the evening’s high-scoring affair as Crewe were hammered 5-2. Marcus Haber marked his full Railwaymen debut with a brace but Done’s treble and goals from Ian Henderson and Peter Vincenti helped Keith Hill’s men to a big away victory. There were plenty of goals between Coventry and Barnsley too in a 2-2 draw with all four goals coming in the first half. Josh McQuoid and Jordan Clarke twice gave the Sky Blues the lead, but the Tykes earned a point thanks to strikes from Conor Hourihane and Leroy Lita. Notts County recorded a 2-1 home win over Colchester with Ronan Murray and Liam Noble on target. Freddie Sears replied for Colchester. James Wilson's second half equaliser earned Oldham a points against Port Vale after Tom Pope's opener and Yeovil claimed a 2-1 away victory at Walsall with Kevin Dawson striking a late winner. Tom Bradshaw had equalised after veteran James Hayter gave the Glovers the lead. Finally, Swindon held Gillingham to a 2-2 draw thanks to Stephen Bywater’s last-minute own goal. Danny Kedwell and Kortney Hause twice gave the Gills the lead but Andy Williams pulled Swindon level before Bywater dropped Raphael Branco's cross into his own net.\n",
            "\n",
            "\n",
            "Summary Text : \n",
            "Fleetwood top of League One after 2-0 win at Scunthorpe .\n",
            "Peterborough, Bristol City, Chesterfield and Crawley all drop first points of the season .\n",
            "Stand-in striker Matt Done scores a hat-trick as Rochdale thrash Crewe 5-2 .\n",
            "Wins for Notts County and Yeovil .\n",
            "Coventry/Bradford and Oldham/Port Vale both end in draws .\n",
            "A late Stephen Bywater own goal denies Gillingham three points against Millwall .\n",
            "===========================================================================================================\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    print(\"Review #\",i+1)\n",
        "    print('Original Text : ')\n",
        "    print(df.article[i])\n",
        "   \n",
        "    print('\\n\\nSummary Text : ')\n",
        "    print(df.highlights[i])\n",
        "    \n",
        "    print('===========================================================================================================\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcid5RGV5nyS"
      },
      "source": [
        "## **Data Pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "we3LgLEC5kWI"
      },
      "outputs": [],
      "source": [
        "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"needn't\": \"need not\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there had\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who's\": \"who is\",\n",
        "\"won't\": \"will not\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you're\": \"you are\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IkizaOFk6Xj8"
      },
      "outputs": [],
      "source": [
        "def clean_text(text, remove_stopwords = True):\n",
        "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
        "    \n",
        "    # Convert words to lower case\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Replace contractions with their longer forms \n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    \n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    \n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        text = text.split()\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stops]\n",
        "        text = \" \".join(text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2uC7Pop7ZOJ",
        "outputId": "1941dd65-9c3a-47de-9b3a-00ca6edf078b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summaries are complete.\n",
            "Texts are complete.\n"
          ]
        }
      ],
      "source": [
        "clean_highlights = []\n",
        "for i in df.highlights:\n",
        "    clean_highlights.append(clean_text(i, remove_stopwords=False))\n",
        "print(\"Summaries are complete.\")\n",
        "\n",
        "clean_article = []\n",
        "for k in df.article:\n",
        "    clean_article.append(clean_text(k))\n",
        "print(\"Texts are complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0-8jsVHCFQt",
        "outputId": "70d997b2-45b3-4e27-cc5f-77739cd7455e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clean Review # 1\n",
            "bishop john folda  of north dakota  is taking time off after being diagnosed   he contracted the infection through contaminated food in italy   church members in fargo  grand forks and jamestown could have been exposed  \n",
            "associated press published 14 11 est 25 october 2013 updated 15 36 est 25 october 2013 bishop fargo catholic diocese north dakota exposed potentially hundreds church members fargo grand forks jamestown hepatitis virus late september early october state health department issued advisory exposure anyone attended five churches took communion bishop john folda pictured fargo catholic diocese north dakota exposed potentially hundreds church members fargo grand forks jamestown hepatitis state immunization program manager molly howell says risk low officials feel important alert people possible exposure diocese announced monday bishop john folda taking time diagnosed hepatitis diocese says contracted infection contaminated food attending conference newly ordained bishops italy last month symptoms hepatitis include fever tiredness loss appetite nausea abdominal discomfort fargo catholic diocese north dakota pictured bishop located\n",
            "\n",
            "Clean Review # 2\n",
            "criminal complaint  cop used his role to help cocaine traffickers   ralph mata  an internal affairs lieutenant  allegedly helped group get guns   he also arranged to pay two assassins in a murder plot  a complaint alleges  \n",
            "cnn ralph mata internal affairs lieutenant miami dade police department working division investigates allegations wrongdoing cops outside office authorities allege 45 year old longtime officer worked drug trafficking organization help plan murder plot get guns criminal complaint unsealed u district court new jersey tuesday accuses mata also known milk man using role police officer help drug trafficking organization exchange money gifts including rolex watch one instance complaint alleges mata arranged pay two assassins kill rival drug dealers killers would pose cops pulling targets shooting according complaint ultimately organization decided move forward murder plot mata still received payment setting meetings federal prosecutors said statement complaint also alleges mata used police badge purchase weapons drug traffickers mata according complaint used contacts airport transport weapons carry luggage trips miami dominican republic court documents released investigators specify name drug trafficking organization mata allegedly conspired says organization importing narcotics places ecuador dominican republic hiding inside shipping containers containing pallets produce including bananas organization distributing narcotics new jersey elsewhere complaint says authorities arrested mata tuesday miami gardens florida immediately clear whether mata attorney police officials could immediately reached comment mata worked miami dade police department since 1992 including directing investigations miami gardens working lieutenant k 9 unit miami international airport according complaint since march 2010 working internal affairs division mata faces charges aiding abetting conspiracy distribute cocaine conspiring distribute cocaine engaging monetary transactions property derived specified unlawful activity scheduled appear federal court florida wednesday convicted mata could face life prison cnn suzanne presto contributed report\n",
            "\n",
            "Clean Review # 3\n",
            "craig eccleston todd  27  had drunk at least three pints before driving car   was using phone when he veered across road in yarmouth  isle of wight   crashed head on into 28 year old rachel titley s car  who died in hospital   police say he would have been over legal drink drive limit at time of crash   he was found guilty at portsmouth crown court of causing death by dangerous driving  \n",
            "drunk driver killed young woman head crash checking mobile phone jailed six years craig eccleston todd 27 driving home night pub received text message reading replying veered across road driving round bend smashed rachel titley’s car coming way craig eccleston todd 27 left using mobile phone crashed head car driven rachel titley 28 right died later injuries head crash took place october 2013 mr eccleston todd car barely recognisable pictured police said eccleston todd drunk least three four pints beer getting behind wheel found guilty causing death dangerous driving portsmouth crown court yesterday miss titley 28 year old solicitor’s clerk cowes isle wight also spent evening friends pub drunk alcohol police said driving responsibly ‘nothing could done avoid collision’ added lindsay pennell prosecuting said ‘craig eccleston todd’s driving resulted tragic death young woman rachel titley death could avoided ‘mr eccleston todd took decision pick mobile phone whilst driving either reading replying text message distracted failed negotiate left hand bend crossing central white line path miss titley’s oncoming car miss titley pulled wreckage daihatsu cuore died later injuries hospital ‘miss titley bright future ahead also returning home spent enjoyable evening friends driving responsibly ‘she arranged contact friends got home confirm arrived safely friends sadly never heard parted company ‘miss titley’s death circumstances reiterates danger using hand held mobile phone whilst driving ’ police unable take breath blood tests eccleston todd immediately tests several hours accident marginally drink drive limit judge agreed police would limit time red citroen hit miss titley’s blue daihatsu cuore road near yarmouth isle wight october 11 2013 phone records showed also texting around time crash pc mark furse hampshire constabulary’s serious collision investigation unit said thoughts rachel family time friends pub shalfleet evening alcohol investigation showed nothing could done avoid collision sadly cost life mr eccleston todd left work yarmouth met friends pub drank least three four pints lager long left pub return home collision occurred around 9 30pm able take breath blood tests immediately although blood taken several hours collision showed marginally limit maintain would limit time collision summing today judge agreed analysis phone records showed texting phone around time collision highly likely would also contributed dangerous driving loss control eccleston todd found guilty causing death dangerous driving following trial portsmouth crown court pictured added mr eccleston todd spend six years behind bars rachel family lost forever hope make people think twice drinking alcohol getting behind wheel using phone road dangers drink driving driving whilst using mobile phone obvious continue risk spending substantial time prison case highlights tragic consequences committing offences ‘mr eccleston todd spend six years behind bars rachel’s family lost ever hope make people think twice drinking alcohol getting behind wheel using phone they’re road case highlights tragic consequences committing offences ’ eccleston todd newport isle wight also disqualified driving eight years complete extended test\n",
            "\n",
            "Clean Review # 4\n",
            "nina dos santos says europe must be ready to accept sanctions will hurt both sides   targeting russia s business community would be one way of sapping their support for president putin  she says   but she says europe would have a hard time keeping its factories going without power from the east  \n",
            "cnn breezy sweep pen president vladimir putin wrote new chapter crimea turbulent history committing region future returned russian domain sixty years prior ukraine breakaway peninsula signed away swiftly soviet leader nikita khrushchev dealing blatant land grab eastern flank anywhere near quick easy europe 28 member union unlike crimea rushed referendum everyone say initially slapping visa restrictions asset freezes limited number little known politicians military men europe facing urgent calls widen scope measures target russian business community particular logic run russia essentially two sides coin alexei navalny one time moscow mayoral contender house arrest opposing current regime called europe leaders ban everyone vladimir putin personal banker chelsea football club owner roman abramovich keeping money loved ones abroad asset freezes visa restrictions especially palatable options eu rolled discretionary basis without requiring cumbersome legal procedures recourse fact russia cancels visas people like time look hermitage capital founder bill browder lost right entry moscow based money 2005 dare go back russia also banned adoption orphans americans retaliation us implementation anti corruption law named sergei magnitsky browder lawyer died year moscow detention center apparently beaten death yet playing money talks card europe must ready consequences action money also walks eu leaders must ready accept sanctions two way street hurt sides targeting russia peripatetic business community would one way sapping tenuous support president putin strategy might also turn silver lining awarding eu countries chance finally deal unpleasant sides patronage including money laundering corruption inflated prize assets like london property picasso paintings years europe hold fire though trade two decades post soviet rapprochement almost 500 billion worth commerce lot put stake true trade war would hurt russia far harder would eu least 15 former gdp comes exports bloc europe hefty reliance russian gas would hard time keeping factories going citizens warm without power east putin flexes political muscle open trade channels keep dialogue going giving sides chance change subject talk less tensely one afford cut lifeline especially europe economy rebound russia one wane\n",
            "\n",
            "Clean Review # 5\n",
            "fleetwood top of league one after 2 0 win at scunthorpe   peterborough  bristol city  chesterfield and crawley all drop first points of the season   stand in striker matt done scores a hat trick as rochdale thrash crewe 5 2   wins for notts county and yeovil   coventry bradford and oldham port vale both end in draws   a late stephen bywater own goal denies gillingham three points against millwall  \n",
            "fleetwood team still 100 record sky bet league one 2 0 win scunthorpe sent graham alexander’s men top table cod army playing third tier first time history six promotions nine years remarkable ascent shows sign slowing jamie proctor gareth evans scoring goals glanford park fleetwood one five teams two two four clubs peterborough bristol city chesterfield crawley hit first stumbling blocks posh defeated 2 1 sheffield united lost opening contests jose baxter’s opener gave blades first half lead although later cancelled shaun brisley’s goal ben davies snatched winner six minutes time lead jose baxter right celebrates opening scoring sheffield united battle sheffield united michael doyle left challenges peterborough kyle vassell keenly contested clash bristol city beat nigel clough’s men opening day held goalless draw last season play finalists leyton orient chesterfield league two champions beaten 1 0 mk dons play manchester united capital one cup seven days’ time arsenal loanee benik afobe scored goal game break meanwhile crawley lost unbeaten status bradford maintained thanks 3 1 win bantams james hanson became first player score crawley season 49 minutes joe walsh equalised five minutes later heads bristol city korey smith left leyton orient lloyd james go header strikes billy knott mason bennett sealed impressive away win phil parkinson men bradford second behind fleetwood doncaster’s stoppage time equaliser meant preston joe garner signed new contract earlier tuesday held 1 1 draw slipped table chris humphrey looked secured points lilywhites nathan tyson struck last gasp leveller stand striker matt done scored hat trick rochdale evening’s high scoring affair crewe hammered 5 2 marcus haber marked full railwaymen debut brace done’s treble goals ian henderson peter vincenti helped keith hill’s men big away victory plenty goals coventry barnsley 2 2 draw four goals coming first half josh mcquoid jordan clarke twice gave sky blues lead tykes earned point thanks strikes conor hourihane leroy lita notts county recorded 2 1 home win colchester ronan murray liam noble target freddie sears replied colchester james wilson second half equaliser earned oldham points port vale tom pope opener yeovil claimed 2 1 away victory walsall kevin dawson striking late winner tom bradshaw equalised veteran james hayter gave glovers lead finally swindon held gillingham 2 2 draw thanks stephen bywater’s last minute goal danny kedwell kortney hause twice gave gills lead andy williams pulled swindon level bywater dropped raphael branco cross net\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    print(\"Clean Review #\",i+1)\n",
        "    print(clean_highlights[i])\n",
        "    print(clean_article[i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "x0GC0H6VCXez"
      },
      "outputs": [],
      "source": [
        "def count_words(count_dict, text):\n",
        "    '''Count the number of occurrences of each word in a set of text'''\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xG9aSgr4Cndg",
        "outputId": "005c1024-b1fe-4112-e35e-d693ee2f3324"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 582374\n"
          ]
        }
      ],
      "source": [
        "# Find the number of times each word was used and the size of the vocabulary\n",
        "word_counts = {}\n",
        "count_words(word_counts, clean_highlights)\n",
        "count_words(word_counts, clean_article)          \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhCS3x8xPk2o",
        "outputId": "1cb75187-74b6-4125-89c5-30bcd9ac82ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word embeddings: 516783\n"
          ]
        }
      ],
      "source": [
        "# Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
        "# (https://github.com/commonsense/conceptnet-numberbatch)\n",
        "embeddings_index = {}\n",
        "with open('/content/numberbatch-en.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwImqDgjPxrI",
        "outputId": "db304f44-9bc6-485e-e69c-6a991fea6405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words missing from CN: 29206\n",
            "Percent of words that are missing from vocabulary: 5.01%\n"
          ]
        }
      ],
      "source": [
        "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
        "missing_words = 0\n",
        "threshold = 20\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
        "            \n",
        "print(\"Number of words missing from CN:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ1QUGDIEgZR",
        "outputId": "8e8779f9-540c-48ed-9150-059f39f71ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of unique words: 582374\n",
            "Number of words we will use: 158807\n",
            "Percent of words we will use: 27.27%\n"
          ]
        }
      ],
      "source": [
        "# Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
        "\n",
        "#dictionary to convert words to integers\n",
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6ChfIbnQHw8",
        "outputId": "4b9d29cb-6efe-4b78-d019-bf9e79031890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "158807\n"
          ]
        }
      ],
      "source": [
        "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "46c5xC4qQUqs"
      },
      "outputs": [],
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    '''Convert words in text to an integer.\n",
        "       If word is not in vocab_to_int, use UNK's integer.\n",
        "       Total the number of words and UNKs.\n",
        "       Add EOS token to the end of texts'''\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngz8yGqnQYUk",
        "outputId": "1645c847-9dea-4e99-b870-73127bee2da7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words in headlines: 128513130\n",
            "Total number of UNKs in headlines: 1296366\n",
            "Percent of words that are UNK: 1.01%\n"
          ]
        }
      ],
      "source": [
        "# Apply convert_to_ints to clean_summaries and clean_texts\n",
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_highlights, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_article, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "hZF55LamRBzh"
      },
      "outputs": [],
      "source": [
        "def create_lengths(text):\n",
        "    '''Create a data frame of the sentence lengths from a text'''\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMXV-iuhRDep",
        "outputId": "241d63b0-7fdd-432a-c1b0-251da51edaf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summaries:\n",
            "              counts\n",
            "count  287113.000000\n",
            "mean       49.508660\n",
            "std        20.819516\n",
            "min         0.000000\n",
            "25%        36.000000\n",
            "50%        46.000000\n",
            "75%        58.000000\n",
            "max      1244.000000\n",
            "\n",
            "Texts:\n",
            "              counts\n",
            "count  287113.000000\n",
            "mean      399.096046\n",
            "std       190.733314\n",
            "min         8.000000\n",
            "25%       259.000000\n",
            "50%       365.000000\n",
            "75%       502.000000\n",
            "max      2001.000000\n"
          ]
        }
      ],
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "print(\"Summaries:\")\n",
        "print(lengths_summaries.describe())\n",
        "print()\n",
        "print(\"Texts:\")\n",
        "print(lengths_texts.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHgcWXu1Sncs",
        "outputId": "78e651dc-2988-4bbf-f2bc-12faeec96872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "665.0\n",
            "778.0\n",
            "961.0\n"
          ]
        }
      ],
      "source": [
        "print(np.percentile(lengths_texts.counts, 90))\n",
        "print(np.percentile(lengths_texts.counts, 95))\n",
        "print(np.percentile(lengths_texts.counts, 99))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31bgy1InSpxE",
        "outputId": "88cb1544-0399-487f-d03d-9795694af218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "74.0\n",
            "87.0\n",
            "116.0\n"
          ]
        }
      ],
      "source": [
        "# Inspect the length of summaries\n",
        "print(np.percentile(lengths_summaries.counts, 90))\n",
        "print(np.percentile(lengths_summaries.counts, 95))\n",
        "print(np.percentile(lengths_summaries.counts, 99))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YzL9Dyn-RT4b"
      },
      "outputs": [],
      "source": [
        "def unk_counter(sentence):\n",
        "    '''Counts the number of time UNK appears in a sentence.'''\n",
        "    count_unk = 0\n",
        "    for i in sentence:\n",
        "      if i == vocab_to_int['<UNK>']:\n",
        "        count_unk += 1\n",
        "    return count_unk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkQ5nELAR-7R",
        "outputId": "5ee53589-cb81-46b3-d03d-4009a16fc982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "227591\n",
            "227591\n"
          ]
        }
      ],
      "source": [
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 959\n",
        "max_summary_length = 116\n",
        "min_length = 8\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in range(min(lengths_texts.counts), max_text_length): \n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "        \n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "        \n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bavRfVmgfu0X"
      },
      "source": [
        "## **Build a Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jKjTN1STeVt3"
      },
      "outputs": [],
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    input_data = tf.compat.v1.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.compat.v1.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.compat.v1.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.compat.v1.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.compat.v1.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.compat.v1.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6XcrYgdje3J_"
      },
      "outputs": [],
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UVhYb6nGfyUL"
      },
      "outputs": [],
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "_OoyG91mgql8"
      },
      "outputs": [],
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "tLIwM8m5gtCI"
      },
      "outputs": [],
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "XZC5VM1dgxMe"
      },
      "outputs": [],
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "            \n",
        "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
        "    #                                                                _zero_state_tensors(rnn_size, \n",
        "    #                                                                                    batch_size, \n",
        "    #                                                                                    tf.float32)) \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.compat.v1.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "MlH1t54Hgy2c"
      },
      "outputs": [],
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uU-vS0CQg3BG"
      },
      "outputs": [],
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "a9Y5h00Ng5VS"
      },
      "outputs": [],
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Bkl_-qpYhCtj"
      },
      "outputs": [],
      "source": [
        "# Set the Hyperparameters\n",
        "epochs = 5 # 100\n",
        "batch_size = 4\n",
        "rnn_size = 256\n",
        "num_layers = 2\n",
        "learning_rate = 0.005\n",
        "keep_probability = 0.75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5PTA-ZjhD34",
        "outputId": "2bdbe509-d4b2-47af-f299-bc1c09bb5107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-30-9390cb26606a>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-30-9390cb26606a>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Graph is built.\n"
          ]
        }
      ],
      "source": [
        "# Build the graph\n",
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXu4nwi-Q-XM"
      },
      "source": [
        "## **Training the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVQRCAI_Q94x",
        "outputId": "e0757295-4b7c-4e2e-bc45-8bf8562be5bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shortest text length: 95\n",
            "The longest text length: 119\n"
          ]
        }
      ],
      "source": [
        "# Subset the data for training\n",
        "start = 2000\n",
        "end = start + 3000\n",
        "sorted_summaries_short = sorted_summaries[start:end]\n",
        "sorted_texts_short = sorted_texts[start:end]\n",
        "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0GXTE8ZTPB9"
      },
      "outputs": [],
      "source": [
        "# Train the Model\n",
        "learning_rate_decay = 0.95\n",
        "min_learning_rate = 0.0005\n",
        "display_step = 20 # Check training loss after every 20 batches\n",
        "stop_early = 0 \n",
        "stop = 6 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
        "per_epoch = 3 # Make 3 update checks per epoch\n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
        "\n",
        "  \n",
        "tf.reset_default_graph()\n",
        "checkpoint = \"best_model.ckpt\"  #300k sentence\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # If we want to continue training a previous session\n",
        "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    # loader.restore(sess, checkpoint)\n",
        "    #sess.run(tf.local_variables_initializer())\n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts_short) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "                \n",
        "                #saver = tf.train.Saver() \n",
        "                #saver.save(sess, checkpoint)\n",
        "                \n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "              \n",
        "                  \n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    # saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMj4xU9wtDOersOK0nGitCq",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1k4DnaAaHil2XQ40f9baWxI3vRkXelq_L",
      "name": "Text summarization pre-processing and training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
